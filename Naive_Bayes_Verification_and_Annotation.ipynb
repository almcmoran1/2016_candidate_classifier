{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Bayes Verification and Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***For my NLP project my goal was to train a candidate classifier using the 2016 presidential debate rhetoric to predict which comment came from which candidate.  My first intuition was to use a Multinomial Naive Bayes model because text data happens to fit very nicely to the bullet points of __Where/When__ to use Naive Bayes.  Due to bag of words featurization of text, the input feature matrix is often very wide (~10,000 - 50,000 columns/dimensions) and can even be greater than the number of data samples (n << p).  In my model I had around 200k features and roughly 2,500 comments.***\n",
    "\n",
    "\n",
    "***Source*** - Jared Thompson lecture, 4.2, 04/17/2106\n",
    "\n",
    "You can look at my project here: https://github.com/almcmoran1/2016_candidate_classifier\n",
    "\n",
    "***The multinomial Bayes algorithm has three main components:***\n",
    "\n",
    "1.) Calculating the prior\n",
    "\n",
    "2.) Building the likelihood probability table from the training data for each label\n",
    "\n",
    "3.) Calculating the log probabilities for each label for your X and taking the max probability.\n",
    "\n",
    "***Starting with 3.) from above, and working backwords, I'll look through sci-kit learn's MultinomialBayes() class and identify where these key parts are being performed.  Following sci-kits API flow, the call to complete 3.) should be embedded in the predict() function.  However, at first glance there doesn't appear to be a predict() function in the Multinomial Class(). Hmmm...that doesn't make much sense...but I do see where the BaseDiscreteNB class is passed as a parameter to Multinomial Class().  Looking through that class, I still do not see the predict function. Following the same logic, I see that the BaseNB class is passed to the BaseDiscrteNB class.  Finally, a predict function!***\n",
    "\n",
    "    python \n",
    "    \"\"\"    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Perform classification on an array of test vectors X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "        Returns\n",
    "        -------\n",
    "        C : array, shape = [n_samples]\n",
    "            Predicted target values for X\n",
    "        \"\"\"\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        return self.classes_[np.argmax(jll, axis=1)]\n",
    "        \"\"\"\n",
    "***We can see here that we are taking the argmax of the output from self._joint_log_likelihood function.  This function is passed from our MultinomialBayes() function.  I've provided an annotation of the code below:***\n",
    "\n",
    "    python\n",
    "    \"\"\"    def _joint_log_likelihood(self, X):\n",
    "        \"\"\"Calculate the posterior log probability of the samples X\"\"\"\n",
    "        check_is_fitted(self, \"classes_\")\n",
    "\n",
    "        X = check_array(X, accept_sparse='csr')\n",
    "        # Aaron's comment begin:\n",
    "        # return a sparse matrix of the sum of the outer product of our input (X) \n",
    "        # and our log probability matrix with the log of the prior\n",
    "        # Aaron's comment end:\n",
    "        return (safe_sparse_dot(X, self.feature_log_prob_.T)\n",
    "                + self.class_log_prior_)\"\"\"\n",
    "\n",
    "***Now all we need to do is identify where we build self.feature_log_prob and self.class_log_prior and we're done! I need to look at the fit() function to see how this is being calculated.  This function is in the BaseDiscreteNB class.  There is some preprocessing checks in this function but the meat of the algorithm is this piece of code***\n",
    "\n",
    "    python\n",
    "    \"\"\"        class_prior = self.class_prior\n",
    "\n",
    "        # Count raw events from data before updating the class log prior\n",
    "        # and feature log probas\n",
    "        n_effective_classes = Y.shape[1]\n",
    "        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n",
    "        self.feature_count_ = np.zeros((n_effective_classes, n_features),\n",
    "                                       dtype=np.float64)\n",
    "        #Building the feature counts and the label counts\n",
    "        self._count(X, Y)\n",
    "        \n",
    "        #Aaron's comment begin:\n",
    "        #Calculating the log probability table\n",
    "        self._update_feature_log_prob()\n",
    "        \n",
    "        #Calcuating the prior value\n",
    "        self._update_class_log_prior(class_prior=class_prior)\n",
    "        return self\"\"\"\n",
    "        \n",
    "***Looking at the self.update_feature_log_prob()***\n",
    "\n",
    "    python\n",
    "    \"\"\"    def _update_feature_log_prob(self):\n",
    "        \"\"\"Apply smoothing to raw counts and recompute log probabilities\"\"\"\n",
    "        smoothed_fc = self.feature_count_ + self.alpha\n",
    "        smoothed_cc = smoothed_fc.sum(axis=1)\n",
    "\n",
    "        # Aaron comment - And here we have our log probability table\n",
    "        \n",
    "        self.feature_log_prob_ = (np.log(smoothed_fc)\n",
    "                                  - np.log(smoothed_cc.reshape(-1, 1)))\n",
    "                                  \"\"\"\n",
    "         \n",
    "***Looking at self._update_class_log_prior to look at the prior:***\n",
    "\n",
    "    python\n",
    "    \"\"\"    def _update_class_log_prior(self, class_prior=None):\n",
    "        n_classes = len(self.classes_)\n",
    "        if class_prior is not None:\n",
    "            if len(class_prior) != n_classes:\n",
    "                raise ValueError(\"Number of priors must match number of\"\n",
    "                                 \" classes.\")\n",
    "                                 \n",
    "            self.class_log_prior_ = np.log(class_prior)\n",
    "        elif self.fit_prior:\n",
    "            # empirical prior, with sample_weight taken into account\n",
    "            \n",
    "            #Aaron comment - Here is were we calculate our prior probability \n",
    "            #(# of classes/total # of classes) and applying log\n",
    "            \n",
    "            self.class_log_prior_ = (np.log(self.class_count_)\n",
    "                                     - np.log(self.class_count_.sum()))\n",
    "        else:\n",
    "            self.class_log_prior_ = np.zeros(n_classes) - np.log(n_classes)\n",
    "            \"\"\"\n",
    "            \n",
    "***And there you have it, the basic pieces to the sci-kit learn algorithm for MultinomialBayes().  I've placed the classes from sci-kit learn below***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultinomialNB(BaseDiscreteNB):\n",
    "    \"\"\"\n",
    "    Naive Bayes classifier for multinomial models\n",
    "    The multinomial Naive Bayes classifier is suitable for classification with\n",
    "    discrete features (e.g., word counts for text classification). The\n",
    "    multinomial distribution normally requires integer feature counts. However,\n",
    "    in practice, fractional counts such as tf-idf may also work.\n",
    "    Read more in the :ref:`User Guide <multinomial_naive_bayes>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float, optional (default=1.0)\n",
    "        Additive (Laplace/Lidstone) smoothing parameter\n",
    "        (0 for no smoothing).\n",
    "    fit_prior : boolean\n",
    "        Whether to learn class prior probabilities or not.\n",
    "        If false, a uniform prior will be used.\n",
    "    class_prior : array-like, size (n_classes,)\n",
    "        Prior probabilities of the classes. If specified the priors are not\n",
    "        adjusted according to the data.\n",
    "    Attributes\n",
    "    ----------\n",
    "    class_log_prior_ : array, shape (n_classes, )\n",
    "        Smoothed empirical log probability for each class.\n",
    "    intercept_ : property\n",
    "        Mirrors ``class_log_prior_`` for interpreting MultinomialNB\n",
    "        as a linear model.\n",
    "    feature_log_prob_ : array, shape (n_classes, n_features)\n",
    "        Empirical log probability of features\n",
    "        given a class, ``P(x_i|y)``.\n",
    "    coef_ : property\n",
    "        Mirrors ``feature_log_prob_`` for interpreting MultinomialNB\n",
    "        as a linear model.\n",
    "    class_count_ : array, shape (n_classes,)\n",
    "        Number of samples encountered for each class during fitting. This\n",
    "        value is weighted by the sample weight when provided.\n",
    "    feature_count_ : array, shape (n_classes, n_features)\n",
    "        Number of samples encountered for each (class, feature)\n",
    "        during fitting. This value is weighted by the sample weight when\n",
    "        provided.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.random.randint(5, size=(6, 100))\n",
    "    >>> y = np.array([1, 2, 3, 4, 5, 6])\n",
    "    >>> from sklearn.naive_bayes import MultinomialNB\n",
    "    >>> clf = MultinomialNB()\n",
    "    >>> clf.fit(X, y)\n",
    "    MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "    >>> print(clf.predict(X[2:3]))\n",
    "    [3]\n",
    "    Notes\n",
    "    -----\n",
    "    For the rationale behind the names `coef_` and `intercept_`, i.e.\n",
    "    naive Bayes as a linear classifier, see J. Rennie et al. (2003),\n",
    "    Tackling the poor assumptions of naive Bayes text classifiers, ICML.\n",
    "    References\n",
    "    ----------\n",
    "    C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\n",
    "    Information Retrieval. Cambridge University Press, pp. 234-265.\n",
    "    http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=1.0, fit_prior=True, class_prior=None):\n",
    "        self.alpha = alpha\n",
    "        self.fit_prior = fit_prior\n",
    "        self.class_prior = class_prior\n",
    "\n",
    "    def _count(self, X, Y):\n",
    "        \"\"\"Count and smooth feature occurrences.\"\"\"\n",
    "        if np.any((X.data if issparse(X) else X) < 0):\n",
    "            raise ValueError(\"Input X must be non-negative\")\n",
    "        self.feature_count_ += safe_sparse_dot(Y.T, X)\n",
    "        self.class_count_ += Y.sum(axis=0)\n",
    "\n",
    "    def _update_feature_log_prob(self):\n",
    "        \"\"\"Apply smoothing to raw counts and recompute log probabilities\"\"\"\n",
    "        smoothed_fc = self.feature_count_ + self.alpha\n",
    "        smoothed_cc = smoothed_fc.sum(axis=1)\n",
    "\n",
    "        self.feature_log_prob_ = (np.log(smoothed_fc)\n",
    "                                  - np.log(smoothed_cc.reshape(-1, 1)))\n",
    "\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        \"\"\"Calculate the posterior log probability of the samples X\"\"\"\n",
    "        check_is_fitted(self, \"classes_\")\n",
    "\n",
    "        X = check_array(X, accept_sparse='csr')\n",
    "        return (safe_sparse_dot(X, self.feature_log_prob_.T)\n",
    "                + self.class_log_prior_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BaseDiscreteNB(BaseNB):\n",
    "    \"\"\"Abstract base class for naive Bayes on discrete/categorical data\n",
    "    Any estimator based on this class should provide:\n",
    "    __init__\n",
    "    _joint_log_likelihood(X) as per BaseNB\n",
    "    \"\"\"\n",
    "\n",
    "    def _update_class_log_prior(self, class_prior=None):\n",
    "        n_classes = len(self.classes_)\n",
    "        if class_prior is not None:\n",
    "            if len(class_prior) != n_classes:\n",
    "                raise ValueError(\"Number of priors must match number of\"\n",
    "                                 \" classes.\")\n",
    "            self.class_log_prior_ = np.log(class_prior)\n",
    "        elif self.fit_prior:\n",
    "            # empirical prior, with sample_weight taken into account\n",
    "            self.class_log_prior_ = (np.log(self.class_count_)\n",
    "                                     - np.log(self.class_count_.sum()))\n",
    "        else:\n",
    "            self.class_log_prior_ = np.zeros(n_classes) - np.log(n_classes)\n",
    "\n",
    "    def partial_fit(self, X, y, classes=None, sample_weight=None):\n",
    "        \"\"\"Incremental fit on a batch of samples.\n",
    "        This method is expected to be called several times consecutively\n",
    "        on different chunks of a dataset so as to implement out-of-core\n",
    "        or online learning.\n",
    "        This is especially useful when the whole dataset is too big to fit in\n",
    "        memory at once.\n",
    "        This method has some performance overhead hence it is better to call\n",
    "        partial_fit on chunks of data that are as large as possible\n",
    "        (as long as fitting in the memory budget) to hide the overhead.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values.\n",
    "        classes : array-like, shape = [n_classes]\n",
    "            List of all the classes that can possibly appear in the y vector.\n",
    "            Must be provided at the first call to partial_fit, can be omitted\n",
    "            in subsequent calls.\n",
    "        sample_weight : array-like, shape = [n_samples], optional\n",
    "            Weights applied to individual samples (1. for unweighted).\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csr', dtype=np.float64)\n",
    "        _, n_features = X.shape\n",
    "\n",
    "        if _check_partial_fit_first_call(self, classes):\n",
    "            # This is the first call to partial_fit:\n",
    "            # initialize various cumulative counters\n",
    "            n_effective_classes = len(classes) if len(classes) > 1 else 2\n",
    "            self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n",
    "            self.feature_count_ = np.zeros((n_effective_classes, n_features),\n",
    "                                           dtype=np.float64)\n",
    "        elif n_features != self.coef_.shape[1]:\n",
    "            msg = \"Number of features %d does not match previous data %d.\"\n",
    "            raise ValueError(msg % (n_features, self.coef_.shape[-1]))\n",
    "\n",
    "        Y = label_binarize(y, classes=self.classes_)\n",
    "        if Y.shape[1] == 1:\n",
    "            Y = np.concatenate((1 - Y, Y), axis=1)\n",
    "\n",
    "        n_samples, n_classes = Y.shape\n",
    "\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            msg = \"X.shape[0]=%d and y.shape[0]=%d are incompatible.\"\n",
    "            raise ValueError(msg % (X.shape[0], y.shape[0]))\n",
    "\n",
    "        # label_binarize() returns arrays with dtype=np.int64.\n",
    "        # We convert it to np.float64 to support sample_weight consistently\n",
    "        Y = Y.astype(np.float64)\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = np.atleast_2d(sample_weight)\n",
    "            Y *= check_array(sample_weight).T\n",
    "\n",
    "        class_prior = self.class_prior\n",
    "\n",
    "        # Count raw events from data before updating the class log prior\n",
    "        # and feature log probas\n",
    "        self._count(X, Y)\n",
    "\n",
    "        # XXX: OPTIM: we could introduce a public finalization method to\n",
    "        # be called by the user explicitly just once after several consecutive\n",
    "        # calls to partial_fit and prior any call to predict[_[log_]proba]\n",
    "        # to avoid computing the smooth log probas at each call to partial fit\n",
    "        self._update_feature_log_prob()\n",
    "        self._update_class_log_prior(class_prior=class_prior)\n",
    "        return self\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Fit Naive Bayes classifier according to X, y\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values.\n",
    "        sample_weight : array-like, shape = [n_samples], optional\n",
    "            Weights applied to individual samples (1. for unweighted).\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        X, y = check_X_y(X, y, 'csr')\n",
    "        _, n_features = X.shape\n",
    "\n",
    "        labelbin = LabelBinarizer()\n",
    "        Y = labelbin.fit_transform(y)\n",
    "        self.classes_ = labelbin.classes_\n",
    "        if Y.shape[1] == 1:\n",
    "            Y = np.concatenate((1 - Y, Y), axis=1)\n",
    "\n",
    "        # LabelBinarizer().fit_transform() returns arrays with dtype=np.int64.\n",
    "        # We convert it to np.float64 to support sample_weight consistently;\n",
    "        # this means we also don't have to cast X to floating point\n",
    "        Y = Y.astype(np.float64)\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = np.atleast_2d(sample_weight)\n",
    "            Y *= check_array(sample_weight).T\n",
    "\n",
    "        class_prior = self.class_prior\n",
    "\n",
    "        # Count raw events from data before updating the class log prior\n",
    "        # and feature log probas\n",
    "        n_effective_classes = Y.shape[1]\n",
    "        self.class_count_ = np.zeros(n_effective_classes, dtype=np.float64)\n",
    "        self.feature_count_ = np.zeros((n_effective_classes, n_features),\n",
    "                                       dtype=np.float64)\n",
    "        self._count(X, Y)\n",
    "        self._update_feature_log_prob()\n",
    "        self._update_class_log_prior(class_prior=class_prior)\n",
    "        return self\n",
    "\n",
    "    # XXX The following is a stopgap measure; we need to set the dimensions\n",
    "    # of class_log_prior_ and feature_log_prob_ correctly.\n",
    "    def _get_coef(self):\n",
    "        return (self.feature_log_prob_[1:]\n",
    "                if len(self.classes_) == 2 else self.feature_log_prob_)\n",
    "\n",
    "    def _get_intercept(self):\n",
    "        return (self.class_log_prior_[1:]\n",
    "                if len(self.classes_) == 2 else self.class_log_prior_)\n",
    "\n",
    "    coef_ = property(_get_coef)\n",
    "    intercept_ = property(_get_intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BaseNB(six.with_metaclass(ABCMeta, BaseEstimator, ClassifierMixin)):\n",
    "    \"\"\"Abstract base class for naive Bayes estimators\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        \"\"\"Compute the unnormalized posterior log probability of X\n",
    "        I.e. ``log P(c) + log P(x|c)`` for all rows x of X, as an array-like of\n",
    "        shape [n_classes, n_samples].\n",
    "        Input is passed to _joint_log_likelihood as-is by predict,\n",
    "        predict_proba and predict_log_proba.\n",
    "        \"\"\"\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Perform classification on an array of test vectors X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "        Returns\n",
    "        -------\n",
    "        C : array, shape = [n_samples]\n",
    "            Predicted target values for X\n",
    "        \"\"\"\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        return self.classes_[np.argmax(jll, axis=1)]\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return log-probability estimates for the test vector X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "        Returns\n",
    "        -------\n",
    "        C : array-like, shape = [n_samples, n_classes]\n",
    "            Returns the log-probability of the samples for each class in\n",
    "            the model. The columns correspond to the classes in sorted\n",
    "            order, as they appear in the attribute `classes_`.\n",
    "        \"\"\"\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        # normalize by P(x) = P(f_1, ..., f_n)\n",
    "        log_prob_x = logsumexp(jll, axis=1)\n",
    "        return jll - np.atleast_2d(log_prob_x).T\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Return probability estimates for the test vector X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "        Returns\n",
    "        -------\n",
    "        C : array-like, shape = [n_samples, n_classes]\n",
    "            Returns the probability of the samples for each class in\n",
    "            the model. The columns correspond to the classes in sorted\n",
    "            order, as they appear in the attribute `classes_`.\n",
    "        \"\"\"\n",
    "        return np.exp(self.predict_log_proba(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
